# Lesson 6: Bilibili è§†é¢‘å­—å¹•æå–ä¸å†…å®¹å¤„ç†

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡ä» Bilibili è§†é¢‘æå–å­—å¹•çš„æŠ€æœ¯æ–¹æ³•
- å­¦ä¹ è§†é¢‘å†…å®¹çš„è‡ªåŠ¨åŒ–å¤„ç†å’Œåˆ†æ
- ç†è§£å­—å¹•æ•°æ®çš„ç»“æ„åŒ–å’Œåº”ç”¨
- å®ç°åŸºäºè§†é¢‘å†…å®¹çš„å­¦ä¹ èµ„æ–™ç”Ÿæˆ

## ğŸ“– ç†è®ºåŸºç¡€

### ä¸ºä»€ä¹ˆéœ€è¦è§†é¢‘å­—å¹•æå–ï¼Ÿ

è§†é¢‘æ•™ç¨‹æ˜¯å­¦ä¹ çš„é‡è¦èµ„æºï¼Œä½†æœ‰ä»¥ä¸‹ç—›ç‚¹ï¼š

1. **å†…å®¹æ£€ç´¢å›°éš¾** - è§†é¢‘å†…å®¹æ— æ³•åƒæ–‡æœ¬ä¸€æ ·æœç´¢
2. **å­¦ä¹ æ•ˆç‡ä½** - éœ€è¦å®Œæ•´è§‚çœ‹æ‰èƒ½æ‰¾åˆ°å…³é”®ä¿¡æ¯
3. **ç¬”è®°æ•´ç†éº»çƒ¦** - æ‰‹åŠ¨è®°å½•è§†é¢‘å†…å®¹è€—æ—¶è€—åŠ›
4. **å¤šè¯­è¨€éšœç¢** - æ— æ³•å¿«é€Ÿç¿»è¯‘æˆ–ç†è§£å¤–è¯­å†…å®¹

### æŠ€æœ¯åŸç†

#### 1. Bilibili å­—å¹•è·å–æœºåˆ¶
```
è§†é¢‘é¡µé¢ â†’ API è°ƒç”¨ â†’ å­—å¹•æ–‡ä»¶ä¸‹è½½ â†’ æ ¼å¼è§£æ â†’ å†…å®¹æå–
```

#### 2. å­—å¹•æ ¼å¼æ ‡å‡†
- **JSON æ ¼å¼** - Bç«™æ ‡å‡†å­—å¹•æ ¼å¼ï¼ŒåŒ…å«æ—¶é—´è½´å’Œæ–‡æœ¬
- **SRT æ ¼å¼** - é€šç”¨å­—å¹•æ ¼å¼ï¼Œä¾¿äºå…¼å®¹å„ç§å·¥å…·
- **ASS æ ¼å¼** - é«˜çº§å­—å¹•æ ¼å¼ï¼Œæ”¯æŒæ ·å¼å’Œç‰¹æ•ˆ

#### 3. å†…å®¹å¤„ç†æµç¨‹
```
åŸå§‹å­—å¹• â†’ æ–‡æœ¬æ¸…æ´— â†’ å†…å®¹åˆ†æ®µ â†’ å…³é”®è¯æå– â†’ ç»“æ„åŒ–æ•´ç† â†’ å­¦ä¹ èµ„æ–™ç”Ÿæˆ
```

## ğŸš€ å®æˆ˜æ¼”ç»ƒ

### æ­¥éª¤ 1: Bilibili å­—å¹•æå–å·¥å…·

åˆ›å»º `bilibili-subtitle-extractor.py`:

```python
#!/usr/bin/env python3
"""
Bilibili è§†é¢‘å­—å¹•æå–å·¥å…·
æ”¯æŒä» B ç«™è§†é¢‘æå–å­—å¹•å¹¶è½¬æ¢ä¸ºå„ç§æ ¼å¼
"""

import requests
import json
import re
import os
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs
from pathlib import Path
import argparse
import srt
import ass


class BilibiliSubtitleExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_video_info(self, url: str) -> Dict:
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        # æå– BV å·
        bvid = self.extract_bvid(url)
        if not bvid:
            raise ValueError("æ— æ³•ä» URL ä¸­æå– BV å·")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        api_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"
        response = self.session.get(api_url)
        response.raise_for_status()
        
        data = response.json()
        if data['code'] != 0:
            raise ValueError(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {data['message']}")
        
        return data['data']
    
    def extract_bvid(self, url: str) -> Optional[str]:
        """ä» URL ä¸­æå– BV å·"""
        patterns = [
            r'BV[a-zA-Z0-9]+',
            r'bvid=([a-zA-Z0-9]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(0) if 'BV' in match.group(0) else match.group(1)
        return None
    
    def get_subtitle_list(self, bvid: str) -> List[Dict]:
        """è·å–å­—å¹•åˆ—è¡¨"""
        api_url = f"https://api.bilibili.com/x/web-interface/v2/aid/info?bvid={bvid}"
        response = self.session.get(api_url)
        response.raise_for_status()
        
        data = response.json()
        if data['code'] != 0:
            return []
        
        # è·å–å­—å¹•ä¿¡æ¯
        video_data = data.get('data', {})
        subtitle_info = video_data.get('subtitle', {})
        
        if not subtitle_info.get('allow_submit', False):
            return []
        
        subtitles = subtitle_info.get('list', [])
        return subtitles
    
    def download_subtitle(self, subtitle_url: str) -> Dict:
        """ä¸‹è½½å­—å¹•æ–‡ä»¶"""
        response = self.session.get(subtitle_url)
        response.raise_for_status()
        return response.json()
    
    def convert_to_srt(self, subtitle_data: Dict) -> str:
        """è½¬æ¢ä¸º SRT æ ¼å¼"""
        srt_subs = []
        
        for i, body in enumerate(subtitle_data.get('body', []), 1):
            start_time = body['from']
            end_time = body['to']
            content = body['content']
            
            # è½¬æ¢æ—¶é—´æ ¼å¼
            start_srt = self.seconds_to_srt_time(start_time)
            end_srt = self.seconds_to_srt_time(end_time)
            
            srt_sub = srt.Subtitle(
                index=i,
                start=srt.srt_timedelta_to_timedelta(start_srt),
                end=srt.srt_timedelta_to_timedelta(end_srt),
                content=content
            )
            srt_subs.append(srt_sub)
        
        return srt.compose(srt_subs)
    
    def convert_to_ass(self, subtitle_data: Dict) -> str:
        """è½¬æ¢ä¸º ASS æ ¼å¼"""
        script = ass.Script()
        
        # æ·»åŠ æ ·å¼
        style = ass.Style(
            name='Default',
            fontname='Arial',
            fontsize=20,
            primarycolor=&HFFFFFF,
            secondarycolor=&H000000,
            outlinecolor=&H000000,
            backcolor=&H80000000,
            bold=0,
            italic=0,
            underline=0,
            strikeout=0,
            scalex=100,
            scaley=100,
            spacing=0,
            angle=0,
            borderstyle=1,
            outline=1,
            shadow=0,
            alignment=2,
            marginl=0,
            marginr=0,
            marginv=0,
            encoding=1
        )
        script.styles.append(style)
        
        # æ·»åŠ å­—å¹•
        for body in subtitle_data.get('body', []):
            start_time = body['from']
            end_time = body['to']
            content = body['content']
            
            event = ass.Dialogue(
                style='Default',
                start=start_time,
                end=end_time,
                text=content
            )
            script.events.append(event)
        
        return str(script)
    
    def seconds_to_srt_time(self, seconds: float) -> srt.srt_timedelta:
        """ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        
        return srt.srt_timedelta(
            hours=hours,
            minutes=minutes,
            seconds=secs,
            milliseconds=milliseconds
        )
    
    def extract_text_content(self, subtitle_data: Dict) -> str:
        """æå–çº¯æ–‡æœ¬å†…å®¹"""
        text_parts = []
        for body in subtitle_data.get('body', []):
            text_parts.append(body['content'])
        return '\n'.join(text_parts)
    
    def process_video(self, url: str, output_dir: str = 'output') -> Dict:
        """å¤„ç†è§†é¢‘å­—å¹•æå–çš„å®Œæ•´æµç¨‹"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = self.get_video_info(url)
        bvid = video_info['bvid']
        title = video_info['title']
        
        print(f"æ­£åœ¨å¤„ç†è§†é¢‘: {title}")
        print(f"BV å·: {bvid}")
        
        # è·å–å­—å¹•åˆ—è¡¨
        subtitles = self.get_subtitle_list(bvid)
        
        if not subtitles:
            print("è¯¥è§†é¢‘æ²¡æœ‰å¯ç”¨çš„å­—å¹•")
            return {}
        
        results = {}
        
        for subtitle in subtitles:
            subtitle_id = subtitle['id']
            subtitle_lang = subtitle['lan']
            subtitle_url = subtitle['subtitle_url']
            
            print(f"æ­£åœ¨ä¸‹è½½å­—å¹•: {subtitle_lang}")
            
            # ä¸‹è½½å­—å¹•
            subtitle_data = self.download_subtitle(subtitle_url)
            
            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', title).strip()
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            
            base_filename = f"{safe_title}_{subtitle_lang}"
            
            # ä¿å­˜å„ç§æ ¼å¼
            formats = {
                'json': lambda data: json.dumps(data, ensure_ascii=False, indent=2),
                'srt': self.convert_to_srt,
                'ass': self.convert_to_ass,
                'txt': self.extract_text_content
            }
            
            format_files = {}
            for format_name, format_func in formats.items():
                filename = f"{base_filename}.{format_name}"
                filepath = Path(output_dir) / filename
                
                try:
                    if format_name == 'json':
                        content = format_func(subtitle_data)
                    else:
                        content = format_func(subtitle_data)
                    
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    format_files[format_name] = str(filepath)
                    print(f"  å·²ä¿å­˜: {filename}")
                    
                except Exception as e:
                    print(f"  ä¿å­˜ {format_name} æ ¼å¼å¤±è´¥: {e}")
            
            results[subtitle_lang] = {
                'subtitle_id': subtitle_id,
                'language': subtitle_lang,
                'files': format_files
            }
        
        return {
            'video_info': video_info,
            'subtitles': results
        }


def main():
    parser = argparse.ArgumentParser(description='Bilibili è§†é¢‘å­—å¹•æå–å·¥å…·')
    parser.add_argument('url', help='Bilibili è§†é¢‘ URL')
    parser.add_argument('-o', '--output', default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--formats', nargs='+', default=['json', 'srt', 'txt'],
                       help='è¾“å‡ºçš„å­—å¹•æ ¼å¼')
    
    args = parser.parse_args()
    
    extractor = BilibiliSubtitleExtractor()
    
    try:
        result = extractor.process_video(args.url, args.output)
        
        if result:
            print("\næå–å®Œæˆ!")
            print(f"è§†é¢‘æ ‡é¢˜: {result['video_info']['title']}")
            print(f"æå–çš„å­—å¹•è¯­è¨€: {list(result['subtitles'].keys())}")
        else:
            print("å­—å¹•æå–å¤±è´¥")
            
    except Exception as e:
        print(f"é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
```

### æ­¥éª¤ 2: å­—å¹•å†…å®¹åˆ†æå·¥å…·

åˆ›å»º `subtitle-analyzer.py`:

```python
#!/usr/bin/env python3
"""
å­—å¹•å†…å®¹åˆ†æå·¥å…·
ç”¨äºåˆ†æå’Œå¤„ç†æå–çš„å­—å¹•å†…å®¹
"""

import json
import re
from typing import Dict, List, Set
from collections import Counter, defaultdict
import jieba
import jieba.analyse
from pathlib import Path


class SubtitleAnalyzer:
    def __init__(self):
        # åŠ è½½ä¸­æ–‡åˆ†è¯è¯å…¸
        jieba.initialize()
    
    def load_subtitle(self, file_path: str) -> Dict:
        """åŠ è½½å­—å¹•æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def extract_keywords(self, subtitle_data: Dict, top_k: int = 20) -> List[tuple]:
        """æå–å…³é”®è¯"""
        text_content = self.extract_text_content(subtitle_data)
        
        # ä½¿ç”¨ TF-IDF æå–å…³é”®è¯
        keywords = jieba.analyse.extract_tags(
            text_content,
            topK=top_k,
            withWeight=True
        )
        
        return keywords
    
    def extract_text_content(self, subtitle_data: Dict) -> str:
        """æå–çº¯æ–‡æœ¬å†…å®¹"""
        text_parts = []
        for body in subtitle_data.get('body', []):
            text_parts.append(body['content'])
        return ' '.join(text_parts)
    
    def analyze_content_structure(self, subtitle_data: Dict) -> Dict:
        """åˆ†æå†…å®¹ç»“æ„"""
        body_list = subtitle_data.get('body', [])
        
        # æ—¶é—´åˆ†æ
        durations = []
        for body in body_list:
            duration = body['to'] - body['from']
            durations.append(duration)
        
        # æ–‡æœ¬é•¿åº¦åˆ†æ
        text_lengths = [len(body['content']) for body in body_list]
        
        # è¯­é€Ÿåˆ†æ (å­—/åˆ†é’Ÿ)
        speech_rates = []
        for i, body in enumerate(body_list):
            duration_minutes = (body['to'] - body['from']) / 60
            if duration_minutes > 0:
                speech_rate = len(body['content']) / duration_minutes
                speech_rates.append(speech_rate)
        
        return {
            'total_segments': len(body_list),
            'total_duration': max(durations) if durations else 0,
            'average_duration': sum(durations) / len(durations) if durations else 0,
            'average_text_length': sum(text_lengths) / len(text_lengths) if text_lengths else 0,
            'average_speech_rate': sum(speech_rates) / len(speech_rates) if speech_rates else 0,
            'content_density': sum(text_lengths) / sum(durations) if durations else 0
        }
    
    def generate_study_notes(self, subtitle_data: Dict) -> str:
        """ç”Ÿæˆå­¦ä¹ ç¬”è®°"""
        keywords = self.extract_keywords(subtitle_data)
        structure = self.analyze_content_structure(subtitle_data)
        text_content = self.extract_text_content(subtitle_data)
        
        notes = f"""# å­¦ä¹ ç¬”è®°

## è§†é¢‘ä¿¡æ¯
- æ€»æ—¶é•¿: {structure['total_duration']:.2f} ç§’
- å­—å¹•æ®µæ•°: {structure['total_segments']}
- å¹³å‡è¯­é€Ÿ: {structure['average_speech_rate']:.2f} å­—/åˆ†é’Ÿ

## å…³é”®è¯
"""
        
        for keyword, weight in keywords[:10]:
            notes += f"- {keyword} (æƒé‡: {weight:.3f})\n"
        
        notes += "\n## å†…å®¹æ‘˜è¦\n"
        
        # ç®€å•çš„å†…å®¹æ‘˜è¦ï¼ˆå–å‰500ä¸ªå­—ç¬¦ï¼‰
        summary = text_content[:500] + "..." if len(text_content) > 500 else text_content
        notes += summary
        
        return notes
    
    def generate_timeline(self, subtitle_data: Dict) -> List[Dict]:
        """ç”Ÿæˆæ—¶é—´çº¿"""
        timeline = []
        
        for body in subtitle_data.get('body', []):
            timeline.append({
                'timestamp': body['from'],
                'time_formatted': self.format_timestamp(body['from']),
                'content': body['content'],
                'duration': body['to'] - body['from']
            })
        
        return timeline
    
    def format_timestamp(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"
    
    def search_content(self, subtitle_data: Dict, query: str) -> List[Dict]:
        """æœç´¢å†…å®¹"""
        results = []
        query_lower = query.lower()
        
        for body in subtitle_data.get('body', []):
            content = body['content']
            if query_lower in content.lower():
                results.append({
                    'timestamp': body['from'],
                    'time_formatted': self.format_timestamp(body['from']),
                    'content': content,
                    'context': self.get_context(subtitle_data, body, 2)
                })
        
        return results
    
    def get_context(self, subtitle_data: Dict, target_body: Dict, context_size: int) -> List[str]:
        """è·å–ä¸Šä¸‹æ–‡"""
        body_list = subtitle_data.get('body', [])
        target_index = -1
        
        for i, body in enumerate(body_list):
            if body == target_body:
                target_index = i
                break
        
        if target_index == -1:
            return []
        
        start_index = max(0, target_index - context_size)
        end_index = min(len(body_list), target_index + context_size + 1)
        
        context = []
        for i in range(start_index, end_index):
            if i != target_index:
                context.append(body_list[i]['content'])
        
        return context


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å­—å¹•å†…å®¹åˆ†æå·¥å…·')
    parser.add_argument('subtitle_file', help='å­—å¹•æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('-o', '--output', default='analysis', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--search', help='æœç´¢å†…å®¹')
    parser.add_argument('--keywords', type=int, default=20, help='å…³é”®è¯æ•°é‡')
    
    args = parser.parse_args()
    
    analyzer = SubtitleAnalyzer()
    
    try:
        # åŠ è½½å­—å¹•æ–‡ä»¶
        subtitle_data = analyzer.load_subtitle(args.subtitle_file)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(args.output).mkdir(parents=True, exist_ok=True)
        
        # åŸºæœ¬åˆ†æ
        structure = analyzer.analyze_content_structure(subtitle_data)
        print("=== å†…å®¹ç»“æ„åˆ†æ ===")
        print(f"æ€»æ®µæ•°: {structure['total_segments']}")
        print(f"æ€»æ—¶é•¿: {structure['total_duration']:.2f} ç§’")
        print(f"å¹³å‡è¯­é€Ÿ: {structure['average_speech_rate']:.2f} å­—/åˆ†é’Ÿ")
        
        # å…³é”®è¯æå–
        keywords = analyzer.extract_keywords(subtitle_data, args.keywords)
        print(f"\n=== å‰{args.keywords}ä¸ªå…³é”®è¯ ===")
        for keyword, weight in keywords:
            print(f"{keyword}: {weight:.3f}")
        
        # ç”Ÿæˆå­¦ä¹ ç¬”è®°
        notes = analyzer.generate_study_notes(subtitle_data)
        notes_file = Path(args.output) / 'study_notes.md'
        with open(notes_file, 'w', encoding='utf-8') as f:
            f.write(notes)
        print(f"\nå­¦ä¹ ç¬”è®°å·²ä¿å­˜: {notes_file}")
        
        # ç”Ÿæˆæ—¶é—´çº¿
        timeline = analyzer.generate_timeline(subtitle_data)
        timeline_file = Path(args.output) / 'timeline.json'
        with open(timeline_file, 'w', encoding='utf-8') as f:
            json.dump(timeline, f, ensure_ascii=False, indent=2)
        print(f"æ—¶é—´çº¿å·²ä¿å­˜: {timeline_file}")
        
        # æœç´¢åŠŸèƒ½
        if args.search:
            results = analyzer.search_content(subtitle_data, args.search)
            print(f"\n=== æœç´¢ç»“æœ: '{args.search}' ===")
            for result in results:
                print(f"[{result['time_formatted']}] {result['content']}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
```

### æ­¥éª¤ 3: å­¦ä¹ èµ„æ–™ç”Ÿæˆå™¨

åˆ›å»º `learning-material-generator.py`:

```python
#!/usr/bin/env python3
"""
å­¦ä¹ èµ„æ–™ç”Ÿæˆå™¨
åŸºäºå­—å¹•å†…å®¹ç”Ÿæˆå„ç§å­¦ä¹ èµ„æ–™
"""

import json
import markdown
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import re


class LearningMaterialGenerator:
    def __init__(self):
        pass
    
    def load_subtitle_data(self, file_path: str) -> Dict:
        """åŠ è½½å­—å¹•æ•°æ®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def generate_quiz_questions(self, subtitle_data: Dict, question_count: int = 10) -> List[Dict]:
        """ç”Ÿæˆæµ‹éªŒé¢˜ç›®"""
        body_list = subtitle_data.get('body', [])
        questions = []
        
        # ç®€å•çš„å…³é”®è¯æå–å’Œé¢˜ç›®ç”Ÿæˆ
        for i in range(0, min(question_count * 3, len(body_list)), 3):
            body = body_list[i]
            content = body['content']
            
            if len(content) > 10:  # åªå¤„ç†æœ‰ä¸€å®šé•¿åº¦çš„å†…å®¹
                # ç”Ÿæˆå¡«ç©ºé¢˜
                question = self.create_fill_in_blank_question(content, body['from'])
                if question:
                    questions.append(question)
                
                if len(questions) >= question_count:
                    break
        
        return questions
    
    def create_fill_in_blank_question(self, content: str, timestamp: float) -> Dict:
        """åˆ›å»ºå¡«ç©ºé¢˜"""
        # ç®€å•çš„å¡«ç©ºé¢˜ç”Ÿæˆé€»è¾‘
        words = re.findall(r'\b[\w\u4e00-\u9fff]+\b', content)
        
        if len(words) >= 3:
            # é€‰æ‹©ä¸­é—´ä½ç½®çš„è¯ä½œä¸ºç­”æ¡ˆ
            answer_word = words[len(words) // 2]
            question_text = content.replace(answer_word, '________')
            
            return {
                'type': 'fill_in_blank',
                'question': question_text,
                'answer': answer_word,
                'timestamp': timestamp,
                'options': [answer_word] + self.generate_distractors(words, answer_word)
            }
        
        return None
    
    def generate_distractors(self, words: List[str], correct_answer: str) -> List[str]:
        """ç”Ÿæˆå¹²æ‰°é¡¹"""
        distractors = []
        for word in words:
            if word != correct_answer and len(word) > 1:
                distractors.append(word)
                if len(distractors) >= 3:
                    break
        
        return distractors
    
    def generate_study_guide(self, subtitle_data: Dict) -> str:
        """ç”Ÿæˆå­¦ä¹ æŒ‡å—"""
        body_list = subtitle_data.get('body', [])
        
        guide = "# å­¦ä¹ æŒ‡å—\n\n"
        guide += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # æŒ‰å†…å®¹åˆ†æ®µ
        sections = self.divide_into_sections(body_list)
        
        for i, section in enumerate(sections, 1):
            guide += f"## ç¬¬{i}éƒ¨åˆ†\n\n"
            
            # æå–å…³é”®ç‚¹
            key_points = self.extract_key_points(section)
            for point in key_points:
                guide += f"- {point}\n"
            
            guide += "\n"
            
            # æ·»åŠ è¯¦ç»†å†…å®¹
            for body in section:
                guide += f"{body['content']}\n"
            
            guide += "\n---\n\n"
        
        return guide
    
    def divide_into_sections(self, body_list: List[Dict], section_size: int = 10) -> List[List[Dict]]:
        """å°†å­—å¹•åˆ†æ®µ"""
        sections = []
        for i in range(0, len(body_list), section_size):
            section = body_list[i:i + section_size]
            sections.append(section)
        return sections
    
    def extract_key_points(self, section: List[Dict]) -> List[str]:
        """æå–å…³é”®ç‚¹"""
        key_points = []
        
        for body in section:
            content = body['content']
            
            # ç®€å•çš„å…³é”®ç‚¹è¯†åˆ«
            if any(keyword in content for keyword in ['é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'è¦ç‚¹', 'æ³¨æ„']):
                key_points.append(content)
            elif len(content) > 20 and 'ã€‚' in content:
                # è¾ƒé•¿çš„å¥å­å¯èƒ½æ˜¯é‡è¦å†…å®¹
                key_points.append(content)
        
        return key_points[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®ç‚¹
    
    def generate_flashcards(self, subtitle_data: Dict) -> List[Dict]:
        """ç”Ÿæˆé—ªå¡"""
        body_list = subtitle_data.get('body', [])
        flashcards = []
        
        for body in body_list:
            content = body['content']
            
            if len(content) > 10 and len(content) < 100:
                # å°†å†…å®¹åˆ†ä¸ºé—®é¢˜å’Œç­”æ¡ˆ
                parts = content.split('ï¼Œ')
                if len(parts) >= 2:
                    question = parts[0].strip() + 'ï¼Ÿ'
                    answer = 'ï¼Œ'.join(parts[1:]).strip()
                    
                    flashcards.append({
                        'front': question,
                        'back': answer,
                        'timestamp': body['from']
                    })
        
        return flashcards
    
    def generate_vocabulary_list(self, subtitle_data: Dict) -> List[Dict]:
        """ç”Ÿæˆè¯æ±‡è¡¨"""
        body_list = subtitle_data.get('body', [])
        vocabulary = []
        
        # ç®€å•çš„è¯æ±‡æå–
        seen_words = set()
        
        for body in body_list:
            content = body['content']
            words = re.findall(r'\b[\w\u4e00-\u9fff]+\b', content)
            
            for word in words:
                if len(word) > 1 and word not in seen_words:
                    vocabulary.append({
                        'word': word,
                        'context': content,
                        'timestamp': body['from']
                    })
                    seen_words.add(word)
        
        return vocabulary
    
    def export_to_anki(self, flashcards: List[Dict], output_file: str):
        """å¯¼å‡ºåˆ° Anki æ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            for card in flashcards:
                # Anki æ ¼å¼: front;back;tags
                f.write(f"{card['front']};{card['back']};video_learning\n")
    
    def export_all_materials(self, subtitle_data: Dict, output_dir: str):
        """å¯¼å‡ºæ‰€æœ‰å­¦ä¹ èµ„æ–™"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 1. å­¦ä¹ æŒ‡å—
        study_guide = self.generate_study_guide(subtitle_data)
        with open(Path(output_dir) / 'study_guide.md', 'w', encoding='utf-8') as f:
            f.write(study_guide)
        
        # 2. æµ‹éªŒé¢˜ç›®
        quiz_questions = self.generate_quiz_questions(subtitle_data)
        with open(Path(output_dir) / 'quiz_questions.json', 'w', encoding='utf-8') as f:
            json.dump(quiz_questions, f, ensure_ascii=False, indent=2)
        
        # 3. é—ªå¡
        flashcards = self.generate_flashcards(subtitle_data)
        with open(Path(output_dir) / 'flashcards.json', 'w', encoding='utf-8') as f:
            json.dump(flashcards, f, ensure_ascii=False, indent=2)
        
        # 4. Anki æ ¼å¼é—ªå¡
        self.export_to_anki(flashcards, Path(output_dir) / 'flashcards_anki.txt')
        
        # 5. è¯æ±‡è¡¨
        vocabulary = self.generate_vocabulary_list(subtitle_data)
        with open(Path(output_dir) / 'vocabulary.json', 'w', encoding='utf-8') as f:
            json.dump(vocabulary, f, ensure_ascii=False, indent=2)
        
        print(f"å­¦ä¹ èµ„æ–™å·²ç”Ÿæˆåˆ°: {output_dir}")
        print(f"- study_guide.md: å­¦ä¹ æŒ‡å—")
        print(f"- quiz_questions.json: æµ‹éªŒé¢˜ç›®")
        print(f"- flashcards.json: é—ªå¡æ•°æ®")
        print(f"- flashcards_anki.txt: Anki é—ªå¡")
        print(f"- vocabulary.json: è¯æ±‡è¡¨")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å­¦ä¹ èµ„æ–™ç”Ÿæˆå™¨')
    parser.add_argument('subtitle_file', help='å­—å¹•æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('-o', '--output', default='learning_materials', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--quiz-count', type=int, default=10, help='æµ‹éªŒé¢˜ç›®æ•°é‡')
    
    args = parser.parse_args()
    
    generator = LearningMaterialGenerator()
    
    try:
        # åŠ è½½å­—å¹•æ•°æ®
        subtitle_data = generator.load_subtitle_data(args.subtitle_file)
        
        # ç”Ÿæˆæ‰€æœ‰å­¦ä¹ èµ„æ–™
        generator.export_all_materials(subtitle_data, args.output)
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
```

### æ­¥éª¤ 4: Claude Code é›†æˆå‘½ä»¤

åˆ›å»º `.claude/commands/bilibili-subtitle-extractor.md`:

```markdown
---
description: ä» Bilibili è§†é¢‘æå–å­—å¹•å¹¶ç”Ÿæˆå­¦ä¹ èµ„æ–™
argument-hint: [Bilibiliè§†é¢‘URL]
allowed-tools: Write, Read, LS, Glob, Grep, Bash(python:*), Bash(curl:*)
---

# Bilibili å­—å¹•æå–å’Œå­¦ä¹ èµ„æ–™ç”Ÿæˆ

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹å¤„ç†ä¸“å®¶ã€‚è¯·å¸®åŠ©ç”¨æˆ·ä» Bilibili è§†é¢‘æå–å­—å¹•ï¼Œå¹¶ç”Ÿæˆå„ç§å­¦ä¹ èµ„æ–™ã€‚

## å·¥ä½œæµç¨‹

### 1. è§†é¢‘ä¿¡æ¯è·å–
- è§£æ Bilibili URLï¼Œæå– BV å·
- è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€UPä¸»ã€æ—¶é•¿ç­‰ï¼‰
- æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰å¯ç”¨å­—å¹•

### 2. å­—å¹•æå–
- ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„å­—å¹•æ–‡ä»¶
- è½¬æ¢ä¸ºå¤šç§æ ¼å¼ï¼ˆJSONã€SRTã€TXTï¼‰
- ä¿å­˜åˆ° organized çš„ç›®å½•ç»“æ„

### 3. å†…å®¹åˆ†æ
- æå–å…³é”®è¯å’Œé‡è¦æ¦‚å¿µ
- åˆ†æå†…å®¹ç»“æ„å’Œè¯­é€Ÿ
- ç”Ÿæˆæ—¶é—´çº¿å’Œå†…å®¹ç´¢å¼•

### 4. å­¦ä¹ èµ„æ–™ç”Ÿæˆ
- ç”Ÿæˆç»“æ„åŒ–çš„å­¦ä¹ ç¬”è®°
- åˆ›å»ºæµ‹éªŒé¢˜ç›®å’Œé—ªå¡
- ç”Ÿæˆè¯æ±‡è¡¨å’Œå­¦ä¹ æŒ‡å—
- å¯¼å‡ºåˆ° Anki å…¼å®¹æ ¼å¼

## æŠ€æœ¯å®ç°

ä½¿ç”¨æä¾›çš„ Python å·¥å…·é“¾ï¼š
- `bilibili-subtitle-extractor.py`: å­—å¹•æå–
- `subtitle-analyzer.py`: å†…å®¹åˆ†æ
- `learning-material-generator.py`: å­¦ä¹ èµ„æ–™ç”Ÿæˆ

## è¾“å‡ºæ ¼å¼

### ç›®å½•ç»“æ„
```
output/
â”œâ”€â”€ video_title/
â”‚   â”œâ”€â”€ subtitles/
â”‚   â”‚   â”œâ”€â”€ video_title_zh.json
â”‚   â”‚   â”œâ”€â”€ video_title_zh.srt
â”‚   â”‚   â””â”€â”€ video_title_zh.txt
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ study_notes.md
â”‚   â”‚   â”œâ”€â”€ timeline.json
â”‚   â”‚   â””â”€â”€ keywords.txt
â”‚   â””â”€â”€ learning_materials/
â”‚       â”œâ”€â”€ study_guide.md
â”‚       â”œâ”€â”€ quiz_questions.json
â”‚       â”œâ”€â”€ flashcards.json
â”‚       â”œâ”€â”€ flashcards_anki.txt
â”‚       â””â”€â”€ vocabulary.json
```

### å­¦ä¹ èµ„æ–™å†…å®¹
- **å­¦ä¹ æŒ‡å—**: ç»“æ„åŒ–çš„å†…å®¹æ€»ç»“å’Œå…³é”®ç‚¹
- **æµ‹éªŒé¢˜ç›®**: åŸºäºå†…å®¹çš„ç†è§£æµ‹è¯•
- **é—ªå¡**: ä¾¿äºè®°å¿†çš„çŸ¥è¯†ç‚¹å¡ç‰‡
- **è¯æ±‡è¡¨**: è§†é¢‘ä¸­çš„é‡è¦è¯æ±‡
- **æ—¶é—´çº¿**: å¸¦æ—¶é—´æˆ³çš„å†…å®¹ç´¢å¼•

## ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬ä½¿ç”¨
/bilibili-subtitle-extractor https://www.bilibili.com/video/BV1HEt4zAEqe

# æŒ‡å®šè¾“å‡ºç›®å½•
/bilibili-subtitle-extractor https://www.bilibili.com/video/BV1HEt4zAEqe -o my_video

# ç”Ÿæˆå­¦ä¹ èµ„æ–™
/bilibili-subtitle-extractor https://www.bilibili.com/video/BV1HEt4zAEqe --generate-materials
```

è¯·åˆ†æä»¥ä¸‹ Bilibili è§†é¢‘ï¼š
$ARGUMENTS
```

## ğŸ“Š åº”ç”¨åœºæ™¯

### 1. è¯­è¨€å­¦ä¹ 
```python
# è¯­è¨€å­¦ä¹ åº”ç”¨
subtitle_analyzer = SubtitleAnalyzer()
subtitle_data = subtitle_analyzer.load_subtitle('video.json')

# æå–è¯æ±‡è¡¨
vocabulary = subtitle_analyzer.generate_vocabulary_list(subtitle_data)

# ç”Ÿæˆè¯­è¨€å­¦ä¹ é—ªå¡
flashcards = subtitle_analyzer.generate_language_flashcards(subtitle_data)
```

### 2. è¯¾ç¨‹ç¬”è®°æ•´ç†
```python
# è‡ªåŠ¨åŒ–è¯¾ç¨‹ç¬”è®°
material_generator = LearningMaterialGenerator()
study_guide = material_generator.generate_study_guide(subtitle_data)

# ç”Ÿæˆå¤ä¹ èµ„æ–™
quiz_questions = material_generator.generate_quiz_questions(subtitle_data)
```

### 3. å†…å®¹æœç´¢å’Œæ£€ç´¢
```python
# å†…å®¹æœç´¢åŠŸèƒ½
search_results = subtitle_analyzer.search_content(subtitle_data, "æœºå™¨å­¦ä¹ ")

# ç”Ÿæˆå†…å®¹ç´¢å¼•
timeline = subtitle_analyzer.generate_timeline(subtitle_data)
```

## ğŸ’¡ é«˜çº§åŠŸèƒ½

### 1. å¤šè¯­è¨€æ”¯æŒ
- è‡ªåŠ¨æ£€æµ‹å­—å¹•è¯­è¨€
- å¤šè¯­è¨€å­—å¹•å¹¶è¡Œå¤„ç†
- è·¨è¯­è¨€å†…å®¹å¯¹æ¯”

### 2. æ™ºèƒ½å†…å®¹åˆ†æ
- å…³é”®æ¦‚å¿µæå–
- çŸ¥è¯†ç‚¹å…³è”åˆ†æ
- éš¾åº¦è¯„ä¼°å’Œåˆ†ç±»

### 3. ä¸ªæ€§åŒ–å­¦ä¹ 
- åŸºäºç”¨æˆ·æ°´å¹³çš„ææ–™ç”Ÿæˆ
- è‡ªé€‚åº”æµ‹éªŒéš¾åº¦
- å­¦ä¹ è¿›åº¦è·Ÿè¸ª

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: æŠ€æœ¯æ•™ç¨‹å­¦ä¹ 

**è§†é¢‘**: Claude Code é«˜çº§æŠ€å·§æ•™ç¨‹

**å¤„ç†æµç¨‹**:
1. æå–å­—å¹•å†…å®¹
2. åˆ†ææŠ€æœ¯å…³é”®è¯
3. ç”Ÿæˆä»£ç ç¤ºä¾‹æ‘˜è¦
4. åˆ›å»ºå®è·µæµ‹éªŒé¢˜ç›®

**è¾“å‡º**:
- æŠ€æœ¯æ¦‚å¿µè¯æ±‡è¡¨
- ä»£ç è¦ç‚¹æ€»ç»“
- å®è·µç»ƒä¹ é¢˜ç›®
- Anki å­¦ä¹ å¡ç‰‡

### æ¡ˆä¾‹ 2: å­¦æœ¯è®²åº§å¤„ç†

**è§†é¢‘**: æœºå™¨å­¦ä¹ ä¸“é¢˜è®²åº§

**å¤„ç†æµç¨‹**:
1. æå–ä¸“ä¸šæœ¯è¯­
2. ç”Ÿæˆæ¦‚å¿µå…³ç³»å›¾
3. åˆ›å»ºç†è®ºæ¡†æ¶ç¬”è®°
4. ç”Ÿæˆå¤ä¹ æµ‹éªŒ

**è¾“å‡º**:
- ä¸“ä¸šæœ¯è¯­è¡¨
- ç†è®ºæ¡†æ¶æ€»ç»“
- æ¦‚å¿µå…³ç³»å›¾
- å­¦æœ¯ç†è§£æµ‹éªŒ

## ğŸ“ ç»ƒä¹ ä½œä¸š

1. **åŸºç¡€ç»ƒä¹ **: ä½¿ç”¨å·¥å…·æå–ä¸€ä¸ª Bilibili è§†é¢‘çš„å­—å¹•ï¼Œå¹¶ç”ŸæˆåŸºæœ¬çš„å­¦ä¹ èµ„æ–™ã€‚

2. **è¿›é˜¶ç»ƒä¹ **: è‡ªå®šä¹‰å­¦ä¹ èµ„æ–™ç”Ÿæˆæ¨¡æ¿ï¼Œé’ˆå¯¹ç‰¹å®šç±»å‹çš„å†…å®¹ï¼ˆå¦‚æŠ€æœ¯æ•™ç¨‹ã€è¯­è¨€å­¦ä¹ ç­‰ï¼‰ã€‚

3. **ç»¼åˆç»ƒä¹ **: åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„è§†é¢‘å­¦ä¹ å·¥ä½œæµï¼Œä»å­—å¹•æå–åˆ°å­¦ä¹ èµ„æ–™ç”Ÿæˆçš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚

4. **åˆ›æ–°åº”ç”¨**: è®¾è®¡ä¸€ä¸ªåŸºäºå­—å¹•å†…å®¹çš„æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å…³äºè§†é¢‘å†…å®¹çš„é—®é¢˜ã€‚

## ğŸ“ æ€»ç»“

### å…³é”®å­¦ä¹ è¦ç‚¹

1. **å­—å¹•æå–æŠ€æœ¯** - æŒæ¡ä»è§†é¢‘å¹³å°æå–å­—å¹•çš„æ–¹æ³•
2. **å†…å®¹åˆ†æèƒ½åŠ›** - ç†è§£å¦‚ä½•ä»æ–‡æœ¬ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯
3. **å­¦ä¹ èµ„æ–™ç”Ÿæˆ** - è‡ªåŠ¨åŒ–åˆ›å»ºå„ç§å­¦ä¹ è¾…åŠ©ææ–™
4. **ä¸ªæ€§åŒ–å­¦ä¹ ** - åŸºäºå†…å®¹ç‰¹ç‚¹å®šåˆ¶å­¦ä¹ ææ–™

### å®é™…åº”ç”¨ä»·å€¼

- **å­¦ä¹ æ•ˆç‡æå‡** - è‡ªåŠ¨åŒ–ç¬”è®°å’Œèµ„æ–™ç”Ÿæˆ
- **å†…å®¹ç»„ç»‡ä¼˜åŒ–** - ç»“æ„åŒ–çš„è§†é¢‘å†…å®¹å¤„ç†
- **çŸ¥è¯†ç®¡ç†** - ç³»ç»ŸåŒ–çš„å­¦ä¹ å’Œå¤ä¹ ææ–™
- **è·¨å¹³å°åº”ç”¨** - é€‚ç”¨äºå„ç§è§†é¢‘å­¦ä¹ å¹³å°

---

**è¯¾ç¨‹å®Œæˆ**: æ­å–œï¼ä½ å·²ç»æŒæ¡äº† Bilibili è§†é¢‘å­—å¹•æå–å’Œå†…å®¹å¤„ç†çš„æŠ€æœ¯ã€‚è¿™äº›æŠ€èƒ½å¯ä»¥å¸®åŠ©ä½ æ›´é«˜æ•ˆåœ°å¤„ç†è§†é¢‘å­¦ä¹ å†…å®¹ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆå­¦ä¹ èµ„æ–™ï¼Œæå‡å­¦ä¹ æ•ˆæœã€‚